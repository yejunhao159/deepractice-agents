# 5.5 Engram 记忆网络：让 AI 有记忆

> 智能的本质是网络的形状

前面几节，我们学习了如何通过角色和工具填补语义鸿沟。但还有一个根本性的问题没有解决：

**每次对话结束，AI 就"失忆"了。**

PromptX 的 **Engram 记忆网络** 正是为解决这个问题而设计的。本节将深入探讨：

- 什么是 Engram（记忆痕迹）
- 语义网络如何工作
- recall-remember 认知循环
- 激活扩散机制
- 记忆的衰减与强化

这是 PromptX 最核心的创新，也是让 AI 真正具备"认知"能力的关键。

---

## 认知科学基础

在深入技术细节之前，我们需要理解人类记忆的工作原理。PromptX 的记忆系统深受认知科学启发。

### 多存储模型（Multi-Store Model）

1968 年，Atkinson 和 Shiffrin 提出了著名的**多存储记忆模型**：

```
感觉输入 → 短期记忆(STM) → [编码/巩固] → 长期记忆(LTM)
                ↑                              │
                └────────── [检索] ────────────┘
```

**核心洞察**：
- **短期记忆** 容量有限（7±2 个项目），持续时间短
- **长期记忆** 容量几乎无限，但需要"编码"才能存入
- 信息在两者之间通过**复述**和**检索**转换

### 情境记忆 vs 语义记忆

1972 年，Tulving 区分了两种长期记忆：

| 类型 | 定义 | 特点 | 例子 |
|-----|------|------|------|
| **情境记忆** | 具体事件、个人经历 | 与时间地点相关 | "昨天用户问了 Redis 缓存的问题" |
| **语义记忆** | 一般知识、概念事实 | 脱离具体情境 | "Redis 是一个内存数据库" |

PromptX 的 Engram 同时支持这两种记忆。

### 遗忘曲线（Forgetting Curve）

1885 年，Ebbinghaus 发现了**遗忘曲线**：

```
记忆保持率
  ^
100%|╲
    |  ╲
 50%|    ╲___
    |        ╲___
 20%|            ╲___________
    |________________________╲___
    0   20min  1h   1d   1w   1m
```

**核心发现**：
- 新学习的内容在最初几分钟到几天遗忘最快
- 随时间推移，遗忘速度逐渐减缓
- **复习** 可以强化记忆，延缓遗忘

### 激活扩散模型（Spreading Activation）

1975 年，Collins 和 Loftus 提出**激活扩散模型**：

```
        动物
       /    \
      狗 ── 猫
     / \    / \
  金毛  柴犬  波斯  英短
```

当你想到"狗"时，相关概念（动物、金毛、柴犬）也会被**激活**。激活程度取决于：
- **关联强度**：关系越紧密，激活越强
- **距离**：越近的概念激活越强
- **时间衰减**：激活会随时间减弱

**这正是 PromptX Engram 记忆网络的理论基础。**

---

## 什么是 Engram

### 神经科学中的 Engram

在神经科学中，**Engram**（记忆痕迹）指的是记忆在大脑中的物理表征——特定神经元集合的突触连接模式。

1904 年，Richard Semon 首次提出这个概念：

> "经验在神经系统中留下持久的变化，这些变化就是 engram。"

### PromptX 中的 Engram

PromptX 借用了这个概念。一个 Engram 包含四个核心属性：

```typescript
interface Engram {
  content: string;    // 记忆内容（感性直观）
  schema: string;     // 概念序列（知性概念化）
  strength: number;   // 记忆强度（0-1）
  type: EngramType;   // 记忆类型
  timestamp: number;  // 创建时间
  id: string;         // 唯一标识
}

type EngramType = 'ATOMIC' | 'LINK' | 'PATTERN';
```

让我们逐一理解：

### 属性一：content（记忆内容）

这是记忆的**原始内容**，保留了感性直观的信息：

```typescript
// 例子
content: "用户使用 PostgreSQL 14，订单表有 500 万行，查询包含多表 JOIN"
```

**特点**：
- 保留完整语义
- 可以是一句话或一段描述
- 是 AI 回答时可以引用的原始素材

### 属性二：schema（概念序列）

这是对 content 的**概念化抽象**，用空格分隔的关键词：

```typescript
// 例子
schema: "用户 技术栈 数据库 PostgreSQL 订单表 JOIN"
```

**作用**：
- 构建语义网络的节点
- 是 recall 检索的索引
- 实现概念间的关联

**生成原则：奥卡姆剃刀**

> "如无必要，勿增实体"

生成 schema 时，删除所有非必要的词汇，只保留传达核心意思的最少元素：

```typescript
// ❌ 冗余的 schema
schema: "用户使用的技术栈是数据库PostgreSQL版本14订单表有500万行"

// ✅ 精简的 schema
schema: "用户 技术栈 PostgreSQL 订单表 JOIN"
```

### 属性三：strength（记忆强度）

0-1 之间的浮点数，表示记忆的**重要程度**：

```typescript
strength: 0.8  // 比较重要的记忆
strength: 0.5  // 一般重要
strength: 0.3  // 不太重要
```

**影响因素**：
- 初始重要性判断
- 被 recall 的次数（越常用越强）
- 时间衰减（越久越弱）

### 属性四：type（记忆类型）

三种 Engram 类型，对应不同的记忆性质：

| 类型 | 含义 | 权重 | 例子 |
|-----|------|------|------|
| **ATOMIC** | 原子概念：具体事实、实体信息 | 1.0 | "用户使用 PostgreSQL 14" |
| **LINK** | 关系连接：偏好关系、因果联系 | 1.5 | "用户偏好 Prisma ORM" |
| **PATTERN** | 模式结构：流程方法、经验模板 | 2.0 | "先优化索引，再分析执行计划" |

**为什么 PATTERN > LINK > ATOMIC？**

- ATOMIC 是基础事实，虽然重要但零散
- LINK 建立了事实之间的关系，增加了结构
- PATTERN 是从多次经验中提炼的智慧，最有价值

---

## 语义网络：Cue 和 Network

Engram 存储的是记忆内容，但记忆之间如何关联？这就需要**语义网络**。

### Cue（线索节点）

Cue 是语义网络中的**节点**，代表一个概念：

```typescript
class Cue {
  word: string;                    // 概念词
  connections: Map<string, number>; // 与其他概念的连接（词 → 权重）

  constructor(word: string) {
    this.word = word;
    this.connections = new Map();
  }

  // 添加或增强连接
  connect(target: string, weight: number = 1) {
    const existing = this.connections.get(target) || 0;
    this.connections.set(target, existing + weight);
  }
}
```

### FrequencyCue（带频率的线索）

FrequencyCue 扩展了 Cue，增加了**召回频率**追踪：

```typescript
class FrequencyCue extends Cue {
  recallCount: number = 0;      // 被召回的次数
  lastRecall: number = 0;       // 最后一次召回时间

  onRecall() {
    this.recallCount++;
    this.lastRecall = Date.now();
  }
}
```

**意义**：经常被使用的概念更容易被想起（频率效应）。

### Network（语义网络）

Network 是 Cue 组成的**图结构**：

```typescript
class Network {
  cues: Map<string, Cue>;  // 所有节点

  constructor() {
    this.cues = new Map();
  }

  // 获取或创建节点
  getOrCreate(word: string): Cue {
    if (!this.cues.has(word)) {
      this.cues.set(word, new Cue(word));
    }
    return this.cues.get(word)!;
  }

  // 建立双向连接
  connect(word1: string, word2: string, weight: number = 1) {
    const cue1 = this.getOrCreate(word1);
    const cue2 = this.getOrCreate(word2);
    cue1.connect(word2, weight);
    cue2.connect(word1, weight);
  }

  // 计算节点的入度（被多少节点指向）
  inDegree(word: string): number {
    let count = 0;
    for (const cue of this.cues.values()) {
      if (cue.connections.has(word)) count++;
    }
    return count;
  }

  // 计算节点的出度（指向多少节点）
  outDegree(word: string): number {
    const cue = this.cues.get(word);
    return cue ? cue.connections.size : 0;
  }
}
```

### 网络的可视化

假设我们存储了以下记忆：

1. "用户使用 PostgreSQL 14，偏好 Prisma ORM"
2. "PostgreSQL 查询优化需要分析执行计划"
3. "用户的订单系统有 500 万数据"

生成的语义网络：

```
                   用户
                  / | \
                 /  |  \
           PostgreSQL-偏好-Prisma
              /  \        |
             /    \       |
        执行计划   查询优化  ORM
             \    /
              \  /
              优化
               |
              订单
               |
             数据量
```

当 AI 需要回答"如何优化 PostgreSQL 查询"时，从"PostgreSQL"和"优化"开始**激活扩散**，能找到：
- "执行计划"（直接连接）
- "用户偏好 Prisma"（通过 PostgreSQL）
- "订单系统 500 万数据"（通过用户）

---

## Remember：存储记忆

### 工作流程

当 AI 需要存储新记忆时：

```typescript
class Remember {
  network: Network;
  engrams: Map<string, Engram>;

  execute(engram: Engram): string {
    // 1. 生成唯一 ID
    const id = `${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
    engram.id = id;

    // 2. 解析 schema 为词列表
    const words = engram.schema.split(/\s+/).filter(w => w.length > 0);

    // 3. 确保所有 Cue 存在
    for (const word of words) {
      this.network.getOrCreate(word);
    }

    // 4. 建立 Cue 之间的连接
    for (let i = 0; i < words.length; i++) {
      for (let j = i + 1; j < words.length; j++) {
        // 计算连接权重（考虑类型和强度）
        const weight = this.calculateWeight(engram);
        this.network.connect(words[i], words[j], weight);
      }
    }

    // 5. 将 engram 关联到各个 Cue
    for (const word of words) {
      const cue = this.network.cues.get(word);
      cue.engrams = cue.engrams || [];
      cue.engrams.push(id);
    }

    // 6. 存储 engram
    this.engrams.set(id, engram);

    return id;
  }

  calculateWeight(engram: Engram): number {
    // 基础权重
    let weight = engram.strength;

    // 类型加成
    const typeMultiplier = {
      'ATOMIC': 1.0,
      'LINK': 1.5,
      'PATTERN': 2.0
    };
    weight *= typeMultiplier[engram.type];

    return weight;
  }
}
```

### 示例

```typescript
// 存储一条记忆
const engram: Engram = {
  content: "用户使用 PostgreSQL 14，团队偏好 Prisma ORM，关注类型安全",
  schema: "用户 PostgreSQL Prisma 类型安全",
  strength: 0.8,
  type: "LINK"
};

const id = remember.execute(engram);
// 网络中新增节点：用户、PostgreSQL、Prisma、类型安全
// 建立连接：用户-PostgreSQL、用户-Prisma、用户-类型安全、PostgreSQL-Prisma、...
```

---

## Recall：召回记忆

### 两阶段召回策略

PromptX 使用**两阶段召回**（Two-Phase Recall）策略：

```
┌─────────────────────────────────────────────────────────────┐
│                    两阶段召回策略                            │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   阶段一：粗召回（Coarse Recall）                            │
│   ┌─────────────────────────────────────────────────────┐  │
│   │                                                     │  │
│   │   1. 接收查询词                                      │  │
│   │   2. 激活扩散（从查询词向外扩散）                      │  │
│   │   3. 收集被激活的所有 Cue                            │  │
│   │   4. 获取关联的 Engram 候选集                        │  │
│   │                                                     │  │
│   │   目标：快速获取可能相关的记忆                        │  │
│   │                                                     │  │
│   └─────────────────────────────────────────────────────┘  │
│                           │                                │
│                           ▼                                │
│   阶段二：精排序（Fine Ranking）                            │
│   ┌─────────────────────────────────────────────────────┐  │
│   │                                                     │  │
│   │   1. 对候选 Engram 计算综合权重                       │  │
│   │   2. 考虑因素：                                      │  │
│   │      • 类型权重（PATTERN > LINK > ATOMIC）           │  │
│   │      • 记忆强度（strength）                          │  │
│   │      • 时间衰减（Ebbinghaus）                        │  │
│   │      • 查询匹配度                                    │  │
│   │   3. 按权重排序                                      │  │
│   │   4. 返回 Top-K 结果                                │  │
│   │                                                     │  │
│   └─────────────────────────────────────────────────────┘  │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 激活扩散机制

PromptX 实现了**海马体式激活扩散**（Hippocampal Activation Spreading）：

```typescript
class HippocampalActivationStrategy {
  firingThreshold: number = 0.1;    // 激活阈值
  synapticDecay: number = 0.9;      // 突触衰减
  inhibitionFactor: number = 0.1;   // 抑制因子
  maxCycles: number = 10;           // 最大扩散轮次
  frequencyBoost: number = 0.1;     // 频率加成

  async activate(
    network: Network,
    seedWords: string[]
  ): Promise<Map<string, number>> {
    // 激活状态：词 → 激活程度
    const activations = new Map<string, number>();

    // 初始激活种子词
    for (const word of seedWords) {
      if (network.cues.has(word)) {
        activations.set(word, 1.0);  // 初始激活为 1.0
      }
    }

    // 迭代扩散
    for (let cycle = 0; cycle < this.maxCycles; cycle++) {
      const newActivations = new Map<string, number>();

      for (const [word, activation] of activations) {
        if (activation < this.firingThreshold) continue;

        const cue = network.cues.get(word);
        if (!cue) continue;

        // 向相邻节点扩散
        for (const [neighbor, connectionWeight] of cue.connections) {
          // 计算扩散的激活量
          const spread = activation * connectionWeight * this.synapticDecay;

          // 频率加成（常用的概念更容易被激活）
          const frequencyBonus = cue instanceof FrequencyCue
            ? cue.recallCount * this.frequencyBoost
            : 0;

          const totalActivation = spread + frequencyBonus;

          // 累加激活（可能有多条路径激活同一个节点）
          const existing = newActivations.get(neighbor) || 0;
          newActivations.set(neighbor, existing + totalActivation);
        }
      }

      // 应用抑制（防止激活爆炸）
      for (const [word, activation] of newActivations) {
        const inhibited = activation * (1 - this.inhibitionFactor);
        const current = activations.get(word) || 0;
        activations.set(word, Math.max(current, inhibited));
      }

      // 检查是否收敛
      if (this.hasConverged(activations, newActivations)) break;
    }

    return activations;
  }
}
```

### DMN 模式：默认模式网络

当 recall 不传入具体查询词时，进入 **DMN 模式**（Default Mode Network）：

```typescript
// 无查询词时
const memories = await recall({
  role: "python-expert",
  query: null  // 或不传
});
```

DMN 模式会：
1. 找出网络中的**枢纽节点**（Hub）—— 连接最多的概念
2. 从枢纽节点开始激活扩散
3. 返回网络的**全景视图**

**枢纽节点选择**：

```typescript
function selectHubNodes(network: Network, count: number = 5): string[] {
  const nodes = Array.from(network.cues.values());

  // 按度数排序（入度 + 出度）
  nodes.sort((a, b) => {
    const degreeA = network.inDegree(a.word) + network.outDegree(a.word);
    const degreeB = network.inDegree(b.word) + network.outDegree(b.word);
    return degreeB - degreeA;
  });

  return nodes.slice(0, count).map(n => n.word);
}
```

**为什么需要 DMN？**

- 用户问"你记得什么？"时，需要返回记忆全景
- AI 接到新任务时，需要先扫描所有相关记忆
- 避免"猜词"失败——先看全景，再精准定位

---

## 认知循环：recall → 思考 → remember

### 完整的工作流程

```
┌─────────────────────────────────────────────────────────────┐
│                    认知循环（Cognitive Loop）                │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   1. 用户提出问题                                            │
│      "如何优化这个 PostgreSQL 查询？"                        │
│            │                                                │
│            ▼                                                │
│   2. DMN 扫描（可选）                                       │
│      recall(role, null) → 获取记忆网络全景                   │
│      发现：用户、PostgreSQL、优化、执行计划...               │
│            │                                                │
│            ▼                                                │
│   3. 精准 recall                                            │
│      recall(role, "PostgreSQL 优化") → 获取相关记忆          │
│      找到：                                                  │
│      - "用户使用 PostgreSQL 14"                             │
│      - "先分析执行计划，再优化索引"                          │
│      - "用户偏好 Prisma ORM"                                │
│            │                                                │
│            ▼                                                │
│   4. 结合记忆 + 预训练知识思考                               │
│      - 记忆提供：用户的具体环境和偏好                        │
│      - 预训练：PostgreSQL 优化的通用知识                     │
│            │                                                │
│            ▼                                                │
│   5. 生成回答                                               │
│      "我记得你使用的是 PostgreSQL 14...                     │
│       建议先运行 EXPLAIN ANALYZE 分析执行计划..."            │
│            │                                                │
│            ▼                                                │
│   6. remember 保存新经验                                    │
│      - 用户的新问题和解决方案                                │
│      - 发现的新偏好或约束                                    │
│      - 值得记住的模式                                        │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

### 实战示例

**第一次对话**：

```
用户：我的订单查询很慢，怎么优化？

AI：(DMN 扫描：记忆网络为空或无相关记忆)

AI：让我先了解一下你的技术栈：
1. 使用什么数据库？
2. 订单表有多少数据？
3. 当前的查询 SQL 是怎样的？

用户：PostgreSQL 14，500万行，有多表 JOIN

AI：(remember)
remember({
  content: "用户使用 PostgreSQL 14，订单表 500 万行，查询包含多表 JOIN",
  schema: "用户 PostgreSQL 订单 500万 JOIN",
  strength: 0.8,
  type: "ATOMIC"
})

AI：了解了。PostgreSQL 14 有一些新的优化特性...
```

**第二次对话（一周后）**：

```
用户：按照你的建议加了索引，但还是慢

AI：(recall "PostgreSQL 优化")
找到记忆："用户使用 PostgreSQL 14，订单表 500 万行，查询包含多表 JOIN"

AI：我记得你使用的是 PostgreSQL 14，订单表有 500 万行。
上次我们讨论了添加索引，现在让我们看看执行计划...

请运行 EXPLAIN ANALYZE 命令，我帮你分析瓶颈在哪里。

用户：[执行计划结果]

AI：(分析后 remember)
remember({
  content: "PostgreSQL 多表 JOIN 优化：先检查索引使用率，再考虑物化视图",
  schema: "PostgreSQL JOIN 优化 索引 物化视图",
  strength: 0.9,
  type: "PATTERN"  // 这是一个模式/方法论
})
```

**第三次对话（一个月后）**：

```
用户：现在又有个库存查询慢的问题

AI：(recall "查询 优化")
找到记忆：
- "用户使用 PostgreSQL 14，订单表 500 万行"
- "PostgreSQL 多表 JOIN 优化：先检查索引使用率..."

AI：我注意到这是你第二次遇到查询性能问题了。
基于之前的经验：
1. 你的技术栈是 PostgreSQL 14
2. 我们发现的模式是：先检查索引使用率，再考虑物化视图

这次的库存查询，我建议我们采取同样的方法...
```

---

## 权重计算：综合评估记忆价值

recall 在精排序阶段需要计算每条记忆的**综合权重**。

### 权重公式

```typescript
function calculateCompositeWeight(
  engram: Engram,
  queryMatch: number,  // 查询匹配度 0-1
  now: number = Date.now()
): number {
  // 1. 类型权重
  const typeWeight = {
    'ATOMIC': 1.0,
    'LINK': 1.5,
    'PATTERN': 2.0
  }[engram.type];

  // 2. 强度权重
  const strengthWeight = engram.strength;

  // 3. 时间衰减（Ebbinghaus 遗忘曲线）
  const daysSinceCreation = (now - engram.timestamp) / (1000 * 60 * 60 * 24);
  const timeDecay = Math.exp(-daysSinceCreation / 30);  // 30天半衰期

  // 4. 查询匹配度
  const matchWeight = queryMatch;

  // 综合计算
  const composite = typeWeight * strengthWeight * timeDecay * matchWeight;

  return composite;
}
```

### 各因素的影响

| 因素 | 影响 | 设计意图 |
|-----|------|---------|
| 类型权重 | PATTERN > LINK > ATOMIC | 方法论比事实更有价值 |
| 强度权重 | 高强度记忆优先 | 重要的事情更容易想起 |
| 时间衰减 | 新记忆权重更高 | 模拟自然遗忘 |
| 匹配度 | 相关性高的优先 | 精准回答问题 |

---

## 记忆的衰减与强化

### 遗忘机制

PromptX 实现了基于 Ebbinghaus 曲线的遗忘：

```typescript
class ForgetMechanism {
  halfLife: number = 30;  // 半衰期（天）

  // 计算当前记忆强度
  getCurrentStrength(engram: Engram): number {
    const daysSinceCreation = this.getDaysSince(engram.timestamp);
    const decayFactor = Math.exp(-daysSinceCreation / this.halfLife);
    return engram.strength * decayFactor;
  }

  // 清理弱记忆
  async cleanup(threshold: number = 0.1): Promise<number> {
    let removed = 0;
    for (const [id, engram] of this.engrams) {
      if (this.getCurrentStrength(engram) < threshold) {
        this.engrams.delete(id);
        removed++;
      }
    }
    return removed;
  }
}
```

### 强化机制

每次 recall 成功时，记忆会被强化：

```typescript
function strengthenOnRecall(engram: Engram): void {
  // 更新最后访问时间
  engram.lastAccess = Date.now();

  // 增加强度（上限 1.0）
  const boostFactor = 1.1;  // 每次增加 10%
  engram.strength = Math.min(1.0, engram.strength * boostFactor);

  // 如果使用 FrequencyCue，更新召回次数
  // 这会影响后续的激活扩散
}
```

**强化的意义**：
- 经常被使用的记忆不会遗忘
- 模拟"间隔重复"学习效果
- 自然形成"核心记忆"和"边缘记忆"

---

## 深层认知：智能的网络本质

PromptX 的创始人 Sean 有一个深刻的认知理论：

> "智能的本质是网络的形状"

### 稀疏网络 vs 密集网络

```
稀疏网络（捕捉大模式）          密集网络（捕捉细微关联）

    A ─────── B                    A ─ B
    │         │                   /│\ │\
    │         │                  / │ \│ \
    │         │                 C ─ D ─ E
    C ─────── D                  \ │ /│ /
                                  \│/ │/
                                   F ─ G

捕捉：因果关系、逻辑推理         捕捉：直觉、感觉、联想
```

### 记忆网络的演化

随着记忆的积累，网络会自然演化：

1. **初期**：稀疏、离散的节点
2. **中期**：形成小的聚类
3. **成熟期**：出现枢纽节点，网络高度互联

**成熟的记忆网络特征**：
- 有明确的枢纽（核心概念）
- 聚类清晰（领域划分）
- 跨领域连接（类比能力）

### 这对 AI 意味着什么

- **RAG 是检索工具，不是认知系统**——RAG 只是"搜索"，Engram 是"想起"
- **记忆的价值在于连接**——孤立的事实不如关联的知识
- **网络越丰富，AI 越"聪明"**——因为能做更多联想

---

## 实战：构建你的记忆网络

### 记忆的最佳实践

**1. 何时 remember？**

| 场景 | 是否记忆 | 原因 |
|-----|---------|------|
| 用户说了技术偏好 | ✅ 记忆 | 这是重要的私有上下文 |
| 解决了一个问题 | ✅ 记忆 | 可能再次遇到类似问题 |
| 发现了一个模式 | ✅ 记忆（PATTERN） | 模式最有价值 |
| 闲聊内容 | ❌ 不记忆 | 没有长期价值 |
| 已知的公共知识 | ❌ 不记忆 | 这是知识污染 |

**2. 如何设计好的 schema？**

```typescript
// ❌ 太长，冗余
schema: "用户的技术栈是PostgreSQL数据库版本14用于订单系统"

// ❌ 太短，丢失信息
schema: "数据库"

// ✅ 恰到好处
schema: "用户 技术栈 PostgreSQL 订单系统"
```

**3. 如何选择 type？**

| 内容 | 类型 | 理由 |
|-----|------|------|
| "用户使用 VSCode" | ATOMIC | 具体事实 |
| "用户喜欢简洁的代码风格" | LINK | 偏好关系 |
| "先写测试，再实现功能" | PATTERN | 方法论 |

**4. 如何设置 strength？**

| 场景 | 建议 strength |
|-----|---------------|
| 用户明确强调的信息 | 0.9 |
| 推断出的偏好 | 0.7 |
| 一般性交互信息 | 0.5 |
| 不确定的猜测 | 0.3 |

### 认知循环的养成

```
每次任务开始：
1. DMN 扫描（recall(role, null)）—— 看看记得什么
2. 从网络图中选词，深入 recall
3. 多轮 recall 直到信息充足

每次任务结束：
4. remember 关键发现
5. 特别注意 PATTERN 类型的记忆
```

**养成这个习惯，记忆网络会越来越丰富！**

---

## Monogent：认知架构的完整图景

前面介绍的 Engram 是记忆的基本单元，但一个完整的认知系统需要更多。**Monogent** 是 PromptX 的认知架构项目，它定义了体验如何被处理、如何演化、以及如何形成连贯的认知链。

> **项目地址**：[github.com/Deepractice/Monogent](https://github.com/Deepractice/Monogent)

### Experience：认知的基本单子

在 Monogent 中，**Experience（体验）** 是认知的原子单位——一个不可再分的认知"单子"（Monad）：

```typescript
interface Experience {
  id: string;           // 唯一标识
  content: any;         // 体验内容（可以是任何形式）
  substrate: Substrate; // 处理基质
  metadata: {
    timestamp: number;
    source: string;     // 来源（用户输入、系统生成等）
    stage: Stage;       // 当前处理阶段
  };
  prev?: Experience;    // 前一个体验（形成链）
  next?: Experience;    // 后一个体验（形成链）
}
```

**为什么是"单子"？**

- **封装性**：体验携带完整的认知上下文
- **不变性**：体验一旦创建，内容不变
- **链接性**：通过 prev/next 形成体验链
- **可组合性**：多个体验可以组合成更复杂的认知结构

### 双基质模型：Computation vs Generation

Monogent 提出了**双基质（Dual Substrate）** 的概念——体验可以在两种不同的"基质"上被处理：

```
┌─────────────────────────────────────────────────────────────┐
│                    双基质模型                                │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   Computation（计算基质）          Generation（生成基质）     │
│   ┌─────────────────────┐       ┌─────────────────────┐   │
│   │                     │       │                     │   │
│   │  • 确定性           │       │  • 概率性           │   │
│   │  • 快速             │       │  • 较慢             │   │
│   │  • 基于算法         │       │  • 基于 LLM         │   │
│   │  • 精确但刚性       │       │  • 灵活但不确定     │   │
│   │                     │       │                     │   │
│   │  适用：             │       │  适用：             │   │
│   │  • 模式匹配         │       │  • 语义理解         │   │
│   │  • 数据变换         │       │  • 创造性生成       │   │
│   │  • 结构化处理       │       │  • 复杂推理         │   │
│   │                     │       │                     │   │
│   └─────────────────────┘       └─────────────────────┘   │
│                                                             │
│              体验在两种基质间流动，选择最合适的处理方式         │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

**设计哲学**：
- 不是所有认知都需要 LLM
- 简单任务用 Computation，节省资源
- 复杂语义任务用 Generation，获得灵活性
- 系统自动选择最优基质

### Evolution：微演化与宏演化

Monogent 定义了两种认知演化方式：

| 类型 | 定义 | 特点 | 例子 |
|-----|------|------|------|
| **微演化** | 在同一 Experience 内的变换 | 内容改变，身份不变 | 文本 → 向量化 |
| **宏演化** | 创建新的 Experience 节点 | 产生新的认知单元 | 理解 → 生成回复 |

```typescript
// 微演化：变换内容，保持同一体验
function microEvolve(exp: Experience, transform: (content: any) => any): Experience {
  return {
    ...exp,
    content: transform(exp.content),
    metadata: { ...exp.metadata, stage: nextStage(exp.metadata.stage) }
  };
}

// 宏演化：创建新体验，链接到当前体验
function macroEvolve(exp: Experience, newContent: any): Experience {
  const newExp: Experience = {
    id: generateId(),
    content: newContent,
    substrate: exp.substrate,
    metadata: { timestamp: Date.now(), source: 'derived', stage: 'INITIAL' },
    prev: exp
  };
  exp.next = newExp;
  return newExp;
}
```

### 七阶段处理管道

Monogent 定义了体验从输入到输出的**七个处理阶段**：

```
感觉 → 知觉 → 表征 → 激活 → 联想 → 回忆 → 整合
(Sensation → Perception → Representation → Activation → Association → Recollection → Integration)
```

每个阶段的职责：

| 阶段 | 职责 | 输入 | 输出 |
|-----|------|------|------|
| **Sensation** | 原始输入接收 | 用户消息、系统事件 | 原始体验 |
| **Perception** | 特征检测与识别 | 原始体验 | 结构化特征 |
| **Representation** | 语义编码 | 结构化特征 | 内部表征 |
| **Activation** | 激活相关记忆节点 | 内部表征 | 激活的 Cue 集合 |
| **Association** | 扩散激活，建立联想 | 激活的 Cue | 相关 Engram 集合 |
| **Recollection** | 召回和排序记忆 | 相关 Engram | 排序后的记忆 |
| **Integration** | 整合记忆与当前输入 | 记忆 + 当前体验 | 完整认知上下文 |

```
┌───────────────────────────────────────────────────────────────────┐
│                     七阶段处理管道                                  │
├───────────────────────────────────────────────────────────────────┤
│                                                                   │
│  用户输入                                                          │
│     │                                                             │
│     ▼                                                             │
│  ┌──────────┐   ┌──────────┐   ┌──────────────┐                 │
│  │ Sensation│ → │Perception│ → │Representation│                 │
│  │ 感觉     │   │ 知觉     │   │ 表征         │                 │
│  └──────────┘   └──────────┘   └──────────────┘                 │
│                                       │                          │
│                                       ▼                          │
│  ┌───────────┐   ┌───────────┐   ┌──────────┐                   │
│  │Integration│ ← │Recollection│ ← │Activation│                   │
│  │ 整合     │   │ 回忆      │   │ 激活      │                   │
│  └───────────┘   └───────────┘   └──────────┘                   │
│       │                               │                          │
│       │                               ▼                          │
│       │                          ┌───────────┐                   │
│       │                          │Association│                   │
│       │                          │ 联想      │                   │
│       │                          └───────────┘                   │
│       ▼                                                          │
│  认知上下文（用于生成回复）                                          │
│                                                                   │
└───────────────────────────────────────────────────────────────────┘
```

### Experience Chain：认知发展的追踪

体验通过 prev/next 指针形成**体验链**，完整记录认知过程：

```
┌─────────┐    ┌─────────┐    ┌─────────┐    ┌─────────┐
│ Exp #1  │ → │ Exp #2  │ → │ Exp #3  │ → │ Exp #4  │
│ 用户问题 │    │ 理解    │    │ 召回    │    │ 回复    │
└─────────┘    └─────────┘    └─────────┘    └─────────┘
   Stage:         Stage:         Stage:         Stage:
 SENSATION    PERCEPTION    RECOLLECTION   INTEGRATION
```

**体验链的价值**：
1. **可追溯性**：可以回溯整个认知过程
2. **可调试性**：发现问题可以定位到具体阶段
3. **可优化性**：分析哪个阶段是瓶颈
4. **上下文保持**：长对话中维持认知连贯性

### 五大认知能力

Monogent 的目标是赋予 AI 五大认知能力：

| 能力 | 定义 | 对应阶段 |
|-----|------|---------|
| **Understanding** | 理解输入的语义 | Perception, Representation |
| **Learning** | 从经验中学习 | Activation, Association |
| **Thinking** | 推理和分析 | Recollection, Integration |
| **Creating** | 生成新内容 | Integration + Generation 基质 |
| **Deciding** | 做出选择 | 全流程 |

### Engram 与 Monogent 的关系

```
┌─────────────────────────────────────────────────────────────┐
│                    架构层次                                   │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│   Monogent（认知架构层）                                     │
│   ├── Experience（认知单子）                                 │
│   ├── Substrate（处理基质）                                  │
│   ├── Evolution（演化机制）                                  │
│   └── Pipeline（处理管道）                                   │
│                │                                            │
│                ▼                                            │
│   Engram（记忆层）                                           │
│   ├── content（记忆内容）                                    │
│   ├── schema（概念索引）                                     │
│   ├── Network（语义网络）                                    │
│   └── recall/remember（认知操作）                            │
│                                                             │
│   关系：Engram 是 Monogent 管道中 Association/Recollection    │
│        阶段的核心数据结构                                     │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

**总结**：
- **Engram** 是记忆的数据结构
- **Monogent** 是完整的认知处理架构
- Engram 是 Monogent 中"记忆"这个维度的实现
- Monogent 还包括感知、理解、推理等更广泛的认知能力

---

## 本节小结

### Engram 记忆痕迹

| 属性 | 作用 |
|-----|------|
| content | 原始记忆内容 |
| schema | 概念索引（空格分隔） |
| strength | 记忆强度（0-1） |
| type | 类型（ATOMIC/LINK/PATTERN） |

### 语义网络

- **Cue**：概念节点，包含与其他概念的连接
- **Network**：Cue 组成的图，支持激活扩散
- **枢纽节点**：连接最多的核心概念

### 认知操作

| 操作 | 作用 |
|-----|------|
| remember | 存储新记忆，构建网络连接 |
| recall | 召回相关记忆，两阶段策略 |
| forget | 自然遗忘，清理弱记忆 |

### 激活扩散

- 从种子词开始向外扩散
- 突触衰减控制扩散范围
- 抑制因子防止激活爆炸
- 频率加成让常用概念更易激活

### 认知循环

```
DMN 全景扫描 → 多轮 recall 深挖 → 思考回答 → remember 保存
```

### Monogent 认知架构

| 概念 | 定义 |
|-----|------|
| Experience | 认知的原子单位（单子） |
| Substrate | 处理基质（Computation / Generation） |
| Evolution | 演化类型（微演化 / 宏演化） |
| Pipeline | 七阶段处理管道 |
| Experience Chain | 体验链，追踪认知发展 |

**Engram 与 Monogent 的关系**：
- Engram 是记忆层的数据结构
- Monogent 是完整的认知架构
- Engram 是 Monogent 管道中 Association/Recollection 阶段的核心实现

---

下一节，我们将回顾整章内容，完成学习检查。

---

[上一节：Luban 工具创建](5.4-Luban工具创建.md) | [返回目录](README.md) | [下一节：本章小结](5.6-本章小结.md)
