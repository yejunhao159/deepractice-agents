# 1.1 什么是智能体？

在探索任何一个复杂概念时，我们最好从一个简洁的定义开始。在人工智能领域，智能体被定义为任何能够通过**传感器（Sensors）**感知其所处**环境（Environment）**，并**自主**地通过**执行器（Actuators）**采取**行动（Action）**以达成特定目标的实体。

这个定义包含了智能体存在的四个基本要素：

1. **环境（Environment）**：智能体所处的外部世界。对于自动驾驶汽车，环境是动态变化的道路交通；对于一个交易算法，环境则是瞬息万变的金融市场。

2. **传感器（Sensors）**：智能体感知环境的方式。摄像头、麦克风、雷达或各类**应用程序编程接口（API）**返回的数据流，都是其感知能力的延伸。

3. **执行器（Actuators）**：智能体改变环境的手段。执行器可以是物理设备（如机械臂、方向盘）或虚拟工具（如执行一段代码、调用一个服务）。

4. **自主性（Autonomy）**：这是赋予智能体"智能"的关键。智能体并非只是被动响应外部刺激或严格执行预设指令的程序，它能够基于其感知和内部状态进行独立决策，以达成其设计目标。

这种从感知到行动的闭环，构成了所有智能体行为的基础，如图 1.1 所示。

<div align="center">
  <img src="https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/1-figures/1757242319667-0.png" alt="图片描述" width="90%"/>
  <p>图 1.1 智能体与环境的基本交互循环</p>
</div>

---

## 1.1.1 传统智能体的演进

在当前**大语言模型（Large Language Model, LLM）**的热潮出现之前，人工智能的先驱们已经对"智能体"这一概念进行了数十年的探索与构建。这些如今我们称之为"传统智能体"的范式，并非单一的静态概念，而是经历了一条从简单到复杂、从被动反应到主动学习的清晰演进路线。

### 反射智能体（Simple Reflex Agent）

这个演进的起点，是那些结构最简单的反射智能体。它们的决策核心由工程师明确设计的"条件-动作"规则构成，如图 1.2 所示。

经典的自动恒温器便是如此：若传感器感知的室温高于设定值，则启动制冷系统。这种智能体完全依赖于当前的感知输入，不具备记忆或预测能力。它像一种数字化的本能，可靠且高效，但也因此无法应对需要理解上下文的复杂任务。

<div align="center">
  <img src="https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/1-figures/1757242319667-1.png" alt="图片描述" width="90%"/>
  <p>图 1.2 简单反射智能体的决策逻辑示意图</p>
</div>

### 基于模型的反射智能体（Model-Based Reflex Agent）

它的局限性引出了一个关键问题：如果环境的当前状态不足以作为决策的全部依据，智能体该怎么办？

为了回答这个问题，研究者们引入了"状态"的概念，发展出基于模型的反射智能体。这类智能体拥有一个内部的**世界模型（World Model）**，用于追踪和理解环境中那些无法被直接感知的方面。它试图回答："世界现在是什么样子的？"

例如，一辆在隧道中行驶的自动驾驶汽车，即便摄像头暂时无法感知到前方的车辆，它的内部模型依然会维持对那辆车存在、速度和预估位置的判断。这个内部模型让智能体拥有了初级的"记忆"，使其决策不再仅仅依赖于瞬时感知，而是基于一个更连贯、更完整的世界状态理解。

### 基于目标的智能体（Goal-Based Agent）

然而，仅仅理解世界还不够，智能体需要有明确的目标。这促进了基于目标的智能体的发展。

与前两者不同，它的行为不再是被动地对环境做出反应，而是主动地、有预见性地选择能够导向某个特定未来状态的行动。这类智能体需要回答的问题是："我应该做什么才能达成目标？"

经典的例子是 GPS 导航系统：你的目标是到达公司，智能体会基于地图数据（世界模型），通过搜索算法（如 A*算法）来规划（Planning）出一条最优路径。这类智能体的核心能力体现在了对未来的考量与规划上。

### 基于效用的智能体（Utility-Based Agent）

更进一步，现实世界的目标往往不是单一的。我们不仅希望到达公司，还希望时间最短、路程最省油并且避开拥堵。

当多个目标需要权衡时，基于效用的智能体便随之出现。它为每一个可能的世界状态都赋予一个效用值，这个值代表了满意度的高低。智能体的核心目标不再是简单地达成某个特定状态，而是最大化期望效用。它需要回答一个更复杂的问题："哪种行为能为我带来最满意的结果？"

这种架构让智能体学会在相互冲突的目标之间进行权衡，使其决策更接近人类的理性选择。

### 学习型智能体（Learning Agent）

至此，我们讨论的智能体虽然功能日益复杂，但其核心决策逻辑——无论是规则、模型还是效用函数——依然依赖于人类设计师的先验知识。如果智能体能不依赖预设，而是通过与环境的互动自主学习呢？

这便是学习型智能体的核心思想，而**强化学习（Reinforcement Learning, RL）**是实现这一思想最具代表性的路径。一个学习型智能体包含一个性能元件（即我们前面讨论的各类智能体）和一个学习元件。学习元件通过观察性能元件在环境中的行动所带来的结果来不断修正性能元件的决策策略。

想象一个学习下棋的 AI。它开始时可能只是随机落子，当它最终赢下一局时，系统会给予它一个正向的奖励。通过大量的自我对弈，学习元件会逐渐发现哪些棋路更有可能导向最终的胜利。**AlphaGo Zero** 是这一理念的一个里程碑式的成就——它在围棋这一复杂博弈中，通过强化学习发现了许多超越人类既有知识的有效策略。

> **知识扩展**：AlphaGo Zero 于 2017 年由 DeepMind 发布，它完全通过自我对弈学习，不使用任何人类棋谱数据。仅用 3 天时间就超越了击败李世石的 AlphaGo Lee 版本，21 天达到 AlphaGo Master 水平，40 天超越所有前代版本。这一成就展示了强化学习在复杂决策任务中的巨大潜力。

从简单的恒温器，到拥有内部模型的汽车，再到能够规划路线的导航、懂得权衡利弊的决策者，最终到可以通过经验自我进化的学习者。这条演进之路，展示了传统人工智能在构建机器智能的道路上所经历的发展脉络。它们为我们今天理解更前沿的智能体范式，打下了坚实而必要的基础。

---

## 1.1.2 大语言模型驱动的新范式

以 **GPT（Generative Pre-trained Transformer）** 为代表的大语言模型的出现，正在显著改变智能体的构建方法与能力边界。

> **最新进展**：2025 年 8 月，OpenAI 发布了 GPT-5，它整合了推理和非推理能力，在数学（AIME 2025 达到 94.6%）、编程（SWE-bench 达到 74.9%）、多模态理解等方面达到了新的性能高度，并显著减少了幻觉问题。GPT-5 现已成为 ChatGPT 的默认模型。

由大语言模型驱动的 LLM 智能体，其核心决策机制与传统智能体存在本质区别，从而赋予了其一系列全新的特性。

这种转变，可以从两者在核心引擎、知识来源、交互方式等多个维度的对比中清晰地看出，如表 1.1 所示。简而言之，传统智能体的能力源于工程师的显式编程与知识构建，其行为模式是确定且有边界的；而 LLM 智能体则通过在海量数据上的预训练，获得了隐式的世界模型与强大的涌现能力，使其能够以更灵活、更通用的方式应对复杂任务。

<div align="center">
  <p>表 1.1 传统智能体与 LLM 驱动智能体的核心对比</p>
  <img src="https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/1-figures/1757242319667-2.png" alt="图片描述" width="90%"/>
</div>

这种差异使得 LLM 智能体可以直接处理高层级、模糊且充满上下文信息的自然语言指令。让我们以一个"智能旅行助手"为例来说明。

在 LLM 智能体出现之前，规划旅行通常意味着用户需要在多个专用应用（如天气、地图、预订网站）之间手动切换，并由用户自己扮演信息整合与决策的角色。而一个 LLM 智能体则能将这个流程整合起来。当接收到"规划一次厦门之旅"这样的模糊指令时，它的工作方式体现了以下几点：

- **规划与推理**：智能体首先会将这个高层级目标分解为一系列逻辑子任务，例如：`[确认出行偏好] -> [查询目的地信息] -> [制定行程草案] -> [预订票务住宿]`。这是一个内在的、由模型驱动的规划过程。
- **工具使用**：在执行规划时，智能体识别到信息缺口，会主动调用外部工具来补全。例如，它会调用天气查询接口获取实时天气，并基于"预报有雨"这一信息，在后续规划中倾向于推荐室内活动。
- **动态修正**：在交互过程中，智能体会将用户的反馈（如"这家酒店超出预算"）视为新的约束，并据此调整后续的行动，重新搜索并推荐符合新要求的选项。

整个"**查天气 → 调行程 → 订酒店**"的流程，展现了其根据上下文动态修正自身行为的能力。

总而言之，我们正从开发专用自动化工具转向构建能自主解决问题的系统。核心不再是编写代码，而是引导一个通用的"大脑"去规划、行动和学习。

---

## 1.1.3 智能体的分类体系

继上文回顾智能体的演进后，本节将从三个互补的维度对智能体进行分类。

### 维度一：基于内部决策架构

第一种分类维度是依据智能体内部决策架构的复杂程度，这个视角在《Artificial Intelligence: A Modern Approach》中系统性地提出[1]。正如 1.1.1 节所述，传统智能体的演进路径本身就构成了最经典的分类阶梯：

- **反应式智能体**：基于条件-动作规则
- **模型式智能体**：引入内部世界模型
- **基于目标的智能体**：具备规划能力
- **基于效用的智能体**：能够权衡多目标
- **学习型智能体**：通过经验自我改进

### 维度二：基于时间与反应性

除了内部架构的复杂性，还可以从智能体处理决策的时间维度进行分类。这个视角关注智能体是在接收到信息后立即行动，还是会经过深思熟虑的规划再行动。这揭示了智能体设计中一个核心权衡：追求速度的**反应性（Reactivity）**与追求最优解的**规划性（Deliberation）**之间的平衡，如图 1.3 所示。

<div align="center">
  <img src="https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/1-figures/1757242319667-3.png" alt="图片描述" width="90%"/>
  <p>图 1.3 智能体决策时间与质量关系图</p>
</div>

#### 反应式智能体 (Reactive Agents)

这类智能体对环境刺激做出近乎即时的响应，决策延迟极低。它们通常遵循从感知到行动的直接映射，不进行或只进行极少的未来规划。

其核心优势在于**速度快、计算开销低**，这在需要快速决策的动态环境中至关重要。例如：
- 车辆的安全气囊系统必须在碰撞发生的毫秒内做出反应
- 高频交易机器人必须依赖反应式决策来捕捉稍纵即逝的市场机会

然而，这种速度的代价是"短视"——由于缺乏长远规划，反应式智能体容易陷入局部最优，难以完成需要多步骤协调的复杂任务。

#### 规划式智能体 (Deliberative Agents)

与反应式智能体相对，规划式（或称审议式）智能体在行动前会进行复杂的思考和规划。它们不会立即对感知做出反应，而是会先利用其内部的世界模型，系统地探索未来的各种可能性，评估不同行动序列的后果，以期找到一条能够达成目标的最佳路径。

可以将其决策过程类比为一位棋手：他不会只看眼前的一步，而是会预想对手可能的应对，并规划出后续几步甚至十几步的棋路。这种深思熟虑的能力使其能够处理复杂的、需要长远眼光的任务，例如制定一份商业计划或规划一次长途旅行。

然而，这种优势的另一面是高昂的时间和计算成本。在瞬息万变的环境中，当规划式智能体还在深思熟虑时，采取行动的最佳时机可能早已过去。

#### 混合式智能体 (Hybrid Agents)

现实世界的复杂任务，往往既需要即时反应，也需要长远规划。例如，智能旅行助手既要能根据用户的即时反馈（如"这家酒店太贵了"）调整推荐（反应性），又要能规划出为期数天的完整旅行方案（规划性）。

一种经典的混合架构是**分层设计**：底层是一个快速的反应模块，处理紧急情况和基本动作；高层则是一个审慎的规划模块，负责制定长远目标。

而现代的 LLM 智能体，则展现了一种更灵活的混合模式。它们通常在一个"思考-行动-观察"的循环中运作：
- **规划 (Reasoning)**：在"思考"阶段，LLM 分析当前状况，规划出下一步的合理行动
- **反应 (Acting & Observing)**：在"行动"和"观察"阶段，智能体与外部工具或环境交互，并立即获得反馈

通过这种方式，智能体将一个需要长远规划的宏大任务，分解为一系列"规划-反应"的微循环，既能灵活应对环境的即时变化，又能通过连贯的步骤完成复杂的长期目标。

### 维度三：基于知识表示

这是一个更根本的分类维度，它探究智能体用以决策的知识，究竟是以何种形式存于其"思想"之中。这个问题是人工智能领域一场持续半个多世纪的辩论核心，并塑造了两种截然不同的 AI 文化。

#### 符号主义 AI（Symbolic AI）

符号主义，常被称为传统人工智能，其核心信念是：**智能源于对符号的逻辑操作**。这里的符号是人类可读的实体（如词语、概念），操作则遵循严格的逻辑规则。这好比一位一丝不苟的图书管理员，将世界知识整理为清晰的规则库和知识图谱。

**优势**：透明和可解释。由于推理步骤明确，其决策过程可以被完整追溯，这在金融、医疗等高风险领域至关重要。

**局限**：它依赖于一个完备的规则体系，但在充满模糊和例外的现实世界中，任何未被覆盖的新情况都可能导致系统失灵——这就是所谓的"知识获取瓶颈"。

#### 亚符号主义 AI（Sub-symbolic AI）

亚符号主义，或称连接主义，则提供了一幅截然不同的图景。在这里，知识并非显式的规则，而是**内隐地分布在一个由大量神经元组成的复杂网络中**，是从海量数据中学习到的统计模式。神经网络和深度学习是其代表。

如果说符号主义 AI 是图书管理员，那么亚符号主义 AI 就像一个牙牙学语的孩童——他不是通过学习"猫有四条腿、毛茸茸、会喵喵叫"这样的规则来认识猫的，而是在看过成千上万张猫的图片后，大脑中的神经网络自然能辨识出"猫"这个概念的视觉模式。

**优势**：强大的模式识别能力和对噪声数据的鲁棒性，能够轻松处理图像、声音等非结构化数据。

**局限**：不透明性。它通常被视为一个**黑箱（Black Box）**——能以惊人的准确率识别出图片中的猫，但无法给出合乎逻辑的解释。此外，它在纯粹的逻辑推理任务上表现不佳，有时会产生看似合理却事实错误的幻觉。

#### 神经符号主义 AI（Neuro-Symbolic AI）

长久以来，符号主义和亚符号主义这两大阵营如同两条平行线，各自发展。为克服两种范式的局限，一种"大和解"的思想开始兴起——**神经符号主义 AI**，也称神经符号混合主义。

它的目标是融合两大范式的优点，创造出一个既能像神经网络一样从数据中学习，又能像符号系统一样进行逻辑推理的混合智能体。

诺贝尔经济学奖得主丹尼尔·卡尼曼在其著作《思考，快与慢》中提出的双系统理论，为我们理解神经符号主义提供了一个绝佳的类比[2]：

- **系统 1**：快速、凭直觉、并行的思维模式，类似于亚符号主义 AI 强大的模式识别能力
- **系统 2**：缓慢、有条理、基于逻辑的审慎思维，恰如符号主义 AI 的推理过程

<div align="center">
  <img src="https://raw.githubusercontent.com/datawhalechina/Hello-Agents/main/docs/images/1-figures/1757242319667-4.png" alt="图片描述" width="90%"/>
  <p>图 1.4 符号主义、亚符号主义与神经符号混合主义的知识表示范式</p>
</div>

人类的智能，正源于这两个系统的协同工作。同样，一个真正鲁棒的 AI，也需要兼具二者之长。

**大语言模型驱动的智能体是神经符号主义的一个极佳实践范例**：其内核是一个巨大的神经网络，使其具备模式识别和语言生成能力；然而，当它工作时，它会生成一系列结构化的中间步骤——如思想、计划或 API 调用——这些都是明确的、可操作的符号。通过这种方式，它实现了感知与认知、直觉与理性的初步融合。

> **2025 年进展**：神经符号 AI 正成为解决 LLM 幻觉问题的重要方向。IBM 等机构正积极研究将符号推理与神经网络结合，以提升 AI 系统的可解释性和可靠性。工具如 Scallop（基于 Datalog 的可微分逻辑推理语言）正在推动这一领域的工程实践。

---

## 参考文献

[1] RUSSELL S, NORVIG P. Artificial Intelligence: A Modern Approach[M]. 4th ed. London: Pearson, 2020.

[2] KAHNEMAN D. Thinking, Fast and Slow[M]. New York: Farrar, Straus and Giroux, 2011.

---

[⬅️ 返回章节目录](README.md) | [➡️ 下一节：智能体如何工作](1.2-智能体如何工作.md)
