# 3.3 大语言模型的能力与边界

在上一节中，我们学习了如何通过提示词与大语言模型"对话"。但仅仅会"对话"是不够的——要构建可靠的智能体，我们必须清楚地知道大语言模型**能做什么、不能做什么**。

这不是学术讨论，而是实战必备：理解能力，才能充分利用；认清边界，才能设计弥补方案。

---

## 3.3.1 大语言模型能做什么

### 能力一：理解和生成自然语言

这是大语言模型最基础也最强大的能力：
- 理解复杂、模糊的自然语言指令
- 生成连贯、流畅的文本内容
- 完成翻译、摘要、改写等文本转换任务

**看个例子**：

```python
# 用户输入可以采用自然语言形式
user_input = "帮我查一下北京这周末的天气,如果适合出门的话,推荐几个公园"

# 大语言模型能够解析出:
# 1. 时间约束:这周末
# 2. 地理位置:北京
# 3. 条件逻辑:如果天气适合出门
# 4. 隐含需求:户外活动 → 公园推荐
```

对比一下传统系统，它需要严格的结构化输入：

```python
# 传统系统需要的参数格式
params = {
    "city": "北京",
    "date_start": "2025-12-20",
    "date_end": "2025-12-21",
    "activity_type": "outdoor",
    "place_category": "park"
}
```

大语言模型的自然语言理解能力显著降低了人机交互的门槛——用户不需要学习特定的指令格式，用日常语言就能表达需求。

### 能力二：广泛的知识储备

大语言模型在万亿token的数据上训练，学到了覆盖各个领域的知识：

```python
# 历史知识
Q: "第二次世界大战何时结束?"
A: "1945年。欧洲战场在5月8日结束,亚洲战场在9月2日日本正式投降时结束。"

# 编程知识
Q: "Python中列表和元组的主要区别是什么?"
A: "列表(list)是可变数据结构,元组(tuple)是不可变数据结构。
    列表使用方括号[],元组使用圆括号()..."

# 常识推理
Q: "下雨天适合户外烧烤活动吗?"
A: "不太适合。降雨会淋湿食材和烧烤设备,且户外环境不舒适,
    存在安全隐患。建议选择有遮蔽的场所或改期进行。"
```

但要注意——模型对通用知识表现良好，对特定领域就不一定了：

```python
# 模型可以回答的通用问题
Q: "FastAPI框架如何实现依赖注入?"
A: "使用Depends()函数..." ✓

# 模型无法准确回答的特定问题
Q: "我们公司内部UserService类的init_db()方法的参数签名是什么?"
A: "根据常见的设计模式..." ✗
# 注:模型没有接触过特定公司的代码,只能基于通用模式推测
```

### 能力三：推理与规划

这是大语言模型最引人注目的**涌现能力**——当模型参数规模超过某个阈值后，自然涌现出的高级认知能力[1]。

**逻辑推理**：

```python
Q: "小明比小红高,小红比小刚高,请问三人中谁最高?"
A: "小明最高。根据传递性推理:小明 > 小红,小红 > 小刚,因此小明 > 小刚。"
```

**因果推理**：

```python
Q: "用户登录系统失败,可能的原因有哪些?"
A: "可能的原因包括:
1. 用户名或密码输入错误
2. 用户账号被锁定或禁用
3. 网络连接异常
4. 认证服务器故障
5. 会话(Session)过期
建议按照由简到繁的顺序逐一排查。"
```

**任务分解与规划**：

```python
Q: "帮我规划一次北京三日游的行程"
A: "建议行程规划如下:

第一天:历史文化探索
- 上午:天安门广场、故宫博物院(预计4小时)
- 下午:景山公园(俯瞰故宫全景)
- 晚上:王府井步行街

第二天:自然与现代景观
- 上午:八达岭长城(往返约4小时)
- 下午:奥林匹克公园(鸟巢、水立方)
- 晚上:三里屯商圈

第三天:艺术与休闲
- 上午:798艺术区
- 下午:颐和园
- 晚上:根据返程时间安排"
```

大语言模型的推理与规划能力是智能体架构的基础——它使智能体能够理解复杂任务、分解为可执行的子步骤、规划合理的执行顺序、根据反馈动态调整策略。

---

## 3.3.2 大语言模型不能做什么

理解局限比理解能力更重要——**局限性决定了智能体架构设计的必要性和方向**。

### 局限一：知识截断

大语言模型只知道训练数据截止日期之前的事情，之后发生的一切它都不知道：

```python
# 假设模型训练数据截止到2023年10月
Q: "2024年巴黎奥运会金牌榜前三名分别是哪些国家?"
A: "抱歉,我的知识库截止到2023年10月,无法提供2024年事件的信息。" ✗
```

**智能体的解决方案**：检索增强生成(RAG)——先用搜索工具获取最新信息，再让模型基于这些信息回答：

```python
# 步骤1: 使用搜索工具检索实时信息
search_result = search_tool("2024巴黎奥运会金牌榜")

# 步骤2: 将检索结果作为上下文注入提示词
prompt = f"""
根据以下最新信息回答问题:

检索结果:
{search_result}

问题:2024年巴黎奥运会金牌榜前三名分别是哪些国家?
"""

# 步骤3: 大语言模型基于最新信息生成答案
answer = llm.generate(prompt)  # ✓ 获得准确的最新答案
```

### 局限二：幻觉

这是大语言模型最臭名昭著的问题：模型可能生成看似合理、表述自信，但实际上完全错误或虚构的信息[2]。

```python
Q: "请推荐北京朝阳区的'天空之镜'咖啡馆"
A: "天空之镜咖啡馆位于北京朝阳区三里屯太古里北区,
   以其独特的镜面天花板设计而闻名,营业时间为10:00-22:00..." ✗

# 真实情况:这家店铺可能根本不存在,名称、地址、特色均为模型编造
```

为什么会产生幻觉？因为大语言模型本质上是概率模型，它生成的是"最可能"的token序列，而不是"最正确"的答案。在信息不足时，模型倾向于基于相似模式进行推测——于是就"编"出来了。

**智能体的解决方案**：通过工具调用获取真实数据，而不是让模型"凭空想象"：

```python
class TravelAgent:
    def recommend_cafe(self, city, keyword):
        # 步骤1: 识别需要外部信息
        thought = "用户需要咖啡馆推荐,应使用搜索工具获取真实信息,而非基于推测"

        # 步骤2: 调用搜索工具获取真实数据
        results = self.search_tool(f"{city} {keyword} 咖啡馆")

        # 步骤3: 基于真实搜索结果生成回答
        if not results:
            return "抱歉,未找到符合条件的咖啡馆信息。"
        else:
            return self.llm.generate(
                f"根据以下搜索结果进行推荐:\n{results}"
            )  # ✓ 基于真实数据,减少幻觉风险
```

### 局限三：计算能力弱

大语言模型在精确数值计算、符号运算等方面表现很差：

```python
Q: "计算 123 × 456 的精确结果"
A: "约等于 56088" ✗
# 正确答案:56088 (此例碰巧正确,但换其他数字常常出错)

Q: "判断114514是否为质数"
A: "是质数" ✗
# 正确答案:不是质数 (114514 = 2 × 57257)

Q: "求解方程 3x² + 5x - 2 = 0"
A: "x ≈ 0.33 或 x ≈ -2" ✗
# 近似正确但不精确
```

原因很简单：大语言模型是语言处理系统，不是计算器。它通过"模式匹配"而非"精确算法"来处理数学问题。

**智能体的解决方案**：让专业的工具做专业的事——遇到计算任务，调用计算器：

```python
class MathAgent:
    def solve(self, expression):
        # 步骤1: 判断是否为计算任务
        if self._is_calculation(expression):
            # 步骤2: 调用计算器工具获得精确结果
            result = self.calculator_tool.eval(expression)  # ✓ 精确计算
            return f"{expression} = {result}"

        # 步骤3: 对于数学推理题,交由大语言模型处理
        elif self._is_math_reasoning(expression):
            return self.llm.solve_reasoning(expression)

        else:
            return "无法识别的数学问题类型"
```

### 局限四：只会说，不会做

大语言模型只能生成文本，无法直接与外部系统交互或执行实际操作：

```python
# 大语言模型只能生成文本内容
Q: "帮我发一封邮件给张经理,内容是本周工作总结"
LLM输出:
"""
邮件草稿如下:
收件人:张经理
主题:本周工作总结

尊敬的张经理:
    本周主要完成了以下工作...
此致
敬礼
"""

# 问题:邮件内容已生成,但并没有真正被发送 ✗
```

**智能体的解决方案**：集成执行器——让模型生成内容，让工具执行操作：

```python
class EmailAgent:
    def send_email(self, recipient, subject, key_points):
        # 步骤1: 使用大语言模型生成邮件内容
        content = self.llm.generate(
            f"撰写一封给{recipient}的{subject}邮件,要点包括:{key_points}"
        )

        # 步骤2: 调用邮件API真正发送邮件
        send_result = self.email_api.send(
            to=recipient,
            subject=subject,
            body=content,
            cc=self.config.get('default_cc')
        )  # ✓ 邮件真正被发送

        # 步骤3: 返回执行状态
        if send_result.success:
            return f"邮件已成功发送至{recipient},消息ID:{send_result.message_id}"
        else:
            return f"邮件发送失败:{send_result.error_message}"
```

---

## 本节小结

大语言模型有三大核心能力：
1. **语言理解与生成**：处理自然语言指令，降低人机交互门槛
2. **广泛的知识储备**：回答各领域问题，但对特定领域有局限
3. **推理与规划**：分解任务、规划步骤，这是最引人注目的涌现能力

也有四大技术局限：
1. **知识截断**：不知道训练数据之后的事情
2. **幻觉**：可能编造看似合理但实际错误的信息
3. **计算能力弱**：精确计算和符号运算表现差
4. **只会说不会做**：只能生成文本，无法执行操作

模型的能力使智能体有了"大脑"，而局限则指明了需要通过架构设计来增强的方向。在下一节中，我们将探讨如何通过智能体架构来弥补这些局限。

---

## 参考文献

[1] Wei J, Tay Y, Bommasani R, et al. Emergent abilities of large language models[J]. arXiv preprint arXiv:2206.07682, 2022.

[2] Ji Z, Lee N, Frieske R, et al. Survey of hallucination in natural language generation[J]. ACM Computing Surveys, 2023, 55(12): 1-38.

---

[⬅️ 上一节:提示工程基础](3.2-Prompt工程基础.md) | [🏠 返回目录](README.md) | [➡️ 下一节:从LLM到智能体架构](3.4-从LLM到智能体架构.md)
