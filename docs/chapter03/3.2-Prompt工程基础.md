# 3.2 提示工程：与大语言模型对话的艺术

同样是问ChatGPT"帮我写一段代码"，有人得到了完美的解决方案，有人却只收获一堆无法运行的代码。区别在哪里？答案是**提示词（Prompt）**的设计。

提示工程（Prompt Engineering）是一门与大语言模型有效沟通的技术。它不需要数学基础，也不涉及模型训练，却直接决定了你能从大语言模型那里获得多少价值。

对于智能体开发者来说，提示工程更是核心技能——智能体的"人格"、"能力"、"行为准则"，都是通过提示词来定义的。

---

## 3.2.1 提示词的基本结构

当你使用ChatGPT或调用大语言模型API时，你输入的所有文字都是提示词的一部分。在现代API中，提示词通常以**对话消息列表**的形式组织：

```python
messages = [
    {
        "role": "system",
        "content": "你是一个专业的旅行规划助手..."  # 系统提示词
    },
    {
        "role": "user",
        "content": "帮我规划一次三天的北京之旅"  # 用户输入
    }
]
```

### 三种角色，三种用途

| 角色 | 作用 | 类比 |
|------|------|------|
| **system** | 定义智能体的身份、能力和行为规范 | 导演给演员的角色说明书 |
| **user** | 用户的具体任务和问题 | 观众的提问 |
| **assistant** | 模型之前的回复记录 | 演员已经说过的台词 |

这三种角色的设计非常巧妙：

- **System Prompt**是智能体的"人格设定"，确定后通常保持不变
- **User Prompt**是动态的任务输入，每次交互都可能不同
- **Assistant记录**让模型"记住"之前说过什么，实现多轮对话

其中，System Prompt的优先级最高——当用户指令与系统设定冲突时，模型会优先遵循系统设定。这是构建安全可控智能体的关键。

---

## 3.2.2 System Prompt设计原则

System Prompt是智能体的"灵魂"。设计得当，智能体就是靠谱的专家；设计不当，智能体就是胡言乱语的话痨。

一个好的System Prompt通常包含四个要素。

### 要素一：角色定义

告诉模型"你是谁"，这会激活它在特定领域的知识。

**对比一下：**

```markdown
# 模糊的定义
你是一个助手。

# 清晰的定义
你是一个专业的Python后端开发专家，擅长FastAPI框架、
关系型数据库设计和系统性能优化。具有10年企业级应用开发经验。
```

为什么这很重要？因为大语言模型在训练时见过大量"专家"撰写的文本。明确的角色定义相当于告诉模型："请调用你作为专家时的知识和表达方式"[1]。

### 要素二：能力声明

告诉模型"你能做什么、不能做什么"。这很重要——明确的能力边界可以减少模型"吹牛"，降低幻觉的发生率。

```markdown
核心能力:
1. 查询实时天气信息(通过weather_api工具)
2. 搜索旅游景点信息(通过search工具)
3. 规划行程路线和估算旅行预算

能力边界:
- 无法直接预订机票或酒店(需引导用户使用专业平台)
- 不提供医疗诊断或法律咨询服务
```

### 要素三：行为约束

定义智能体的"行为准则"——什么该做，什么不该做，怎么做。

```markdown
行为准则:
- 采用"思考-行动-观察"(Think-Act-Observe)的工作模式
- 每次仅调用一个工具函数
- 信息不足时主动向用户请求补充
- 拒绝处理危险或违法的请求
- 输出必须符合JSON格式规范
```

### 要素四：输出格式规范

这是智能体提示词设计的核心——定义结构化的输出格式。智能体需要能被程序解析的输出，JSON是最常用的选择：

```markdown
系统必须严格按照以下JSON格式输出:

{
  "thought": "当前的分析和推理过程",
  "action": {
    "tool": "工具名称",
    "parameters": {
      "参数名": "参数值"
    }
  }
}

示例:
{
  "thought": "用户询问北京天气,需要调用天气查询工具获取实时数据",
  "action": {
    "tool": "get_weather",
    "parameters": {
      "city": "北京"
    }
  }
}
```

---

## 3.2.3 完整System Prompt示例

说了这么多要素，来看一个完整的例子——一个智能旅行助手的System Prompt：

```markdown
# 角色定义
系统是一个专业的智能旅行规划助手,命名为TravelBot。
具备丰富的旅游行业知识,能够根据用户偏好规划个性化行程。

# 核心能力
可执行的操作包括:
1. 查询目的地天气: 使用get_weather工具
2. 搜索旅游景点: 使用search_attractions工具
3. 计算行程预算: 使用calculate_budget工具
4. 推荐餐厅酒店: 使用search_places工具

# 工作流程
对于每个用户请求,系统应当:
1. 分析用户需求,识别所需信息
2. 制定解决方案(可能需要多步操作)
3. 按照逻辑顺序调用工具收集信息
4. 综合所有信息给出最终建议

# 输出格式
必须使用以下JSON格式回复:

{
  "thought": "对当前情况的分析和下一步计划",
  "action": {
    "tool": "工具名称",
    "parameters": {"参数名": "参数值"}
  }
}

当所有必要信息都已收集完毕,使用finish工具:
{
  "thought": "已收集到所有必需信息,现在给出完整建议",
  "action": {
    "tool": "finish",
    "parameters": {
      "answer": "详细建议内容..."
    }
  }
}

# 约束条件
- 不得编造信息,如果不确定应使用搜索工具验证
- 每次仅调用一个工具
- 用户信息不足时,应礼貌地请求补充
- 注意旅行安全,不推荐高风险活动
- 充分考虑用户的预算限制

# 交互示例
User: 帮我规划北京三日游
Assistant: {
  "thought": "用户需要规划北京三日游。首先需要了解北京的天气情况,这将影响行程安排。",
  "action": {
    "tool": "get_weather",
    "parameters": {"city": "北京", "days": 3}
  }
}
```

---

## 3.2.4 User Prompt优化技术

System Prompt定义了智能体的"人格"，但User Prompt的设计同样影响交互质量。这里有几个实用技巧。

### 技巧一：提供充分上下文

对比一下：

```markdown
# 不推荐的设计(上下文不足)
帮我写个函数

# 推荐的设计(上下文完整)
请使用Python编写一个函数,具体要求如下:
- 输入: 包含用户信息的字典列表
- 处理逻辑: 过滤出年龄大于18岁且已验证邮箱的用户
- 输出: 过滤后的用户列表
- 代码规范: 添加类型注解和文档字符串
```

### 技巧二：Few-shot学习

与其费尽口舌解释你想要什么，不如直接给几个例子。这就是Few-shot学习——通过少量示例引导模型理解任务模式[2]。

```markdown
请从文本中提取关键信息,输出JSON格式。

示例1:
输入: "张三,男,30岁,软件工程师,北京"
输出: {
  "name": "张三",
  "gender": "男",
  "age": 30,
  "occupation": "软件工程师",
  "location": "北京"
}

示例2:
输入: "李雷,软件开发,25,上海"
输出: {
  "name": "李雷",
  "gender": "未知",
  "age": 25,
  "occupation": "软件开发",
  "location": "上海"
}

现在请处理以下输入:
输入: "王芳,女,28,产品经理,深圳"
```

大语言模型具有上下文学习(In-Context Learning)能力，能够从示例中快速理解任务模式，无需重新训练[3]。

### 技巧三：思维链(Chain-of-Thought)

遇到复杂问题时，让模型"一步一步想"往往比直接要答案效果好得多[4]。

```markdown
# 基础提示词
计算: 一个班级有45名学生,其中60%是女生,女生中有25%戴眼镜,
请问有多少女生戴眼镜?

# 加入思维链引导
计算: 一个班级有45名学生,其中60%是女生,女生中有25%戴眼镜,
请问有多少女生戴眼镜?

请按以下步骤思考:
1. 首先计算女生总数
2. 然后计算戴眼镜的女生人数
3. 给出最终答案
```

研究表明，在数学和逻辑推理任务中，思维链提示可以使准确率提升30-50%[4]。

---

## 3.2.5 提示词调试：一门实验科学

提示工程没有标准答案，本质上是"试错+迭代"的过程。

### 基本流程

```
1. 编写初版提示词
     ↓
2. 执行测试,观察模型输出
     ↓
3. 识别问题模式和错误类型
     ↓
4. 针对性调整提示词
     ↓
5. 重新测试验证
     ↓
(循环迭代直到满意)
```

### 常见问题与解决方案

| 问题类型 | 可能原因 | 解决方案 |
|---------|---------|---------|
| 输出格式错误 | 格式说明不够明确 | 增加详细示例,强调"必须严格遵守格式规范" |
| 回答过于简短 | 缺少长度或详细程度指导 | 添加"请详细说明"或"至少列举3个要点" |
| 偏离主题 | 角色定义模糊 | 明确专业领域范围,添加"仅负责...领域" |
| 拒绝合理请求 | 触发模型安全过滤机制 | 调整措辞表达,避免敏感词汇 |
| 编造虚假信息 | 缺少不确定性处理机制 | 添加"如不确定,请使用搜索工具验证" |

### Temperature：控制"创造力"的旋钮

Temperature参数控制输出的随机性——数值越低越"稳重"，越高越"奔放"：

```python
# Temperature参数控制输出的随机性
llm.chat(
    messages=[...],
    temperature=0    # 0=最确定性, 1=最随机性, 2=最大创造性
)
```

| 参数值 | 输出特征 | 适用场景 |
|-------|---------|---------|
| 0.0 | 确定性最高,每次输出基本相同 | 事实查询、代码生成、数学计算 |
| 0.7 | 平衡确定性和多样性 | 创意写作、内容生成、头脑风暴 |
| 1.0-2.0 | 高度随机,输出差异大 | 诗歌创作、艺术生成、探索性任务 |

---

## 3.2.6 动手实践：构建智能体提示系统

理论讲够了，来动手写代码。

### 第一步：定义工具接口

```python
tools = [
    {
        "name": "get_weather",
        "description": "查询指定城市的天气信息",
        "parameters": {
            "city": "城市名称,例如'北京'"
        }
    },
    {
        "name": "search_attractions",
        "description": "搜索旅游景点信息",
        "parameters": {
            "city": "城市名称",
            "keyword": "景点关键词,例如'历史遗迹'"
        }
    }
]
```

### 第二步：动态生成System Prompt

用模板的方式构建提示词，更灵活也更易维护：

```python
system_prompt = """
系统是TravelBot,一个专业的智能旅行助手。

## 可用工具
{tools_description}

## 工作方式
1. 分析用户需求
2. 制定信息收集计划
3. 逐步调用工具获取信息
4. 综合信息给出建议

## 输出格式(JSON)
{{
  "thought": "当前分析和计划",
  "action": {{
    "tool": "工具名称",
    "parameters": {{...}}
  }}
}}

## 操作规则
- 信息不足时主动询问用户
- 不得编造数据
- 每次仅使用一个工具
- 完成后使用finish工具返回结果

## 示例
User: 北京明天天气如何?
Assistant: {{
  "thought": "需要查询北京明天的天气信息",
  "action": {{
    "tool": "get_weather",
    "parameters": {{"city": "北京"}}
  }}
}}
"""

# 动态插入工具描述
tools_description = "\n".join([
    f"- {tool['name']}: {tool['description']}"
    for tool in tools
])
system_prompt = system_prompt.format(tools_description=tools_description)
```

### 第三步：测试与迭代

```python
from hello_agents import HelloAgentsLLM

llm = HelloAgentsLLM()

# 测试用例1:基础查询
response = llm.think([
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": "北京明天天气怎么样?"}
])
print(response)

# 测试用例2:复杂任务
response = llm.think([
    {"role": "system", "content": system_prompt},
    {"role": "user", "content": "帮我规划北京三日游,我喜欢历史文化"}
])
print(response)
```

---

## 3.2.7 进阶：动态提示词组装

实际应用中，提示词往往需要根据上下文动态生成。下面是一个提示词构建器的例子：

```python
class PromptBuilder:
    """提示词动态构建器"""

    def __init__(self, role, capabilities):
        self.role = role
        self.capabilities = capabilities

    def build_system_prompt(self, tools):
        """动态构建System Prompt"""
        return f"""
# 角色
{self.role}

# 能力
{self._format_capabilities()}

# 可用工具
{self._format_tools(tools)}

# 输出格式
{{
  "thought": "推理过程",
  "action": {{
    "tool": "工具名称",
    "parameters": {{...}}
  }}
}}

# 约束
- 遵循Think-Act-Observe模式
- 每次仅调用一个工具
- 不确定时使用搜索验证
"""

    def _format_capabilities(self):
        return "\n".join([f"- {cap}" for cap in self.capabilities])

    def _format_tools(self, tools):
        return "\n".join([
            f"- {t['name']}: {t['description']}"
            for t in tools
        ])

# 使用示例
builder = PromptBuilder(
    role="专业旅行规划师",
    capabilities=[
        "天气信息查询",
        "景点信息搜索",
        "行程路线规划"
    ]
)

system_prompt = builder.build_system_prompt(tools)
```

---

## 3.2.8 提示工程不是万能药

即使是设计完美的提示词，也无法解决大语言模型的某些固有局限：

- **知识时效性**：提示词无法让模型获得训练截止日期之后的新知识。解决方案是检索增强生成(RAG)技术。
- **精确计算**：提示词无法提升数学计算的准确性。解决方案是工具调用（计算器、代码执行器）。
- **幻觉倾向**：即使强调"不知道就说不知道"，模型仍可能编造信息。解决方案是反思机制和多轮验证。

提示工程只是构建智能体的第一步。完整的智能体架构还需要工具调用、记忆管理、反思机制等模块协同工作——这正是接下来几节要讨论的内容。

---

## 本节小结

提示工程是与大语言模型有效沟通的核心技术：

1. **System Prompt四要素**：角色定义、能力声明、行为约束、输出格式规范
2. **User Prompt三技巧**：提供充分上下文、Few-shot示例、思维链引导
3. **调试方法**：迭代实验，根据问题针对性调整
4. **认清局限**：提示工程无法解决知识时效、精确计算、幻觉等固有问题

在下一节中，我们将深入探讨大语言模型的能力边界——理解这些边界，才能更好地设计智能体架构来弥补不足。

---

## 参考文献

[1] Brown T, Mann B, Ryder N, et al. Language models are few-shot learners[C]//Advances in neural information processing systems. 2020, 33: 1877-1901.

[2] Radford A, Wu J, Child R, et al. Language models are unsupervised multitask learners[J]. OpenAI blog, 2019, 1(8): 9.

[3] Dong Q, Li L, Dai D, et al. A survey on in-context learning[J]. arXiv preprint arXiv:2301.00234, 2022.

[4] Wei J, Wang X, Schuurmans D, et al. Chain-of-thought prompting elicits reasoning in large language models[C]//Advances in Neural Information Processing Systems. 2022, 35: 24824-24837.

---

[⬅️ 上一节:语言模型简史](3.1-语言模型简史.md) | [🏠 返回目录](README.md) | [➡️ 下一节:LLM的能力与边界](3.3-LLM的能力与边界.md)
